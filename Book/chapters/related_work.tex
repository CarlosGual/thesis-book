\chapter{Related Work}\label{ch:related-work}

As the fields of \acrshort{vsn} \acrshort{rl} are very broad, this chapter aims to provide an extensive and organized overview of the most relevant works in the literature.
More specifically, this chapter delves into the main topics discussed alongside this thesis.
Since the main focus of this work is within robotic navigation, \acrlong{vsn} is the first topic covered.
It will lay the groundwork for how the other topics are related to robotic navigation and how they can be applied.
The following section will cover the sparsity and exploration methods, which are crucial for addressing the challenges of sparse rewards and exploration in \acrshort{rl}.
The next section will discuss the simulation-to-reality transfer in robotic navigation, which is essential for deploying models trained in simulation to real robots.
The subsequent section will cover offline \acrshort{rl}, which is a paradigm that allows training models using fixed datasets, without the need for online data collection.
Finally, the last section will discuss meta \acrshort{rl}, which focuses on learning policies that can quickly adapt to new environments.

While section~\ref{sec:visual-semantic-navigation} applies to the whole content of this document, the rest of the sections are more focused on the specific contributions of this thesis:

\begin{itemize}
    \item Section~\ref{sec:sparsity-and-exploration-methods} refers to chapter~\ref{ch:understanding-robotic-visual-semantic-navigation}, where we explore exploration methods based on the use of procedural environments and simulated environments.
    \item Section~\ref{sec:simulation-to-reality-transfer-in-robotic-navigation} is related to chapter~\ref{ch:ros4vsn:-enable-real-world-robotic-visual-semantic-navigation}, where we present the ROS4\acrshort{vsn} library, which allows the integration of different \acrshort{vsn} models in real robots.
    \item Section~\ref{sec:offline-reinforcement-learning} is related to chapter~\ref{sec:offline_rl4rvsn}, where we explore the use of offline \acrshort{rl} techniques to train navigation policies in simulated environments.
    \item Section~\ref{sec:meta-reinforcement-learning} is related to chapter~\ref{sec:mil-for-real-world-navigation}, where we explore the use of meta \acrshort{rl} techniques to train navigation policies that can quickly adapt to new environments.
\end{itemize}

\section{Visual Semantic Navigation}\label{sec:visual-semantic-navigation}

\acrfull{vsn} is a subfield of robotic navigation that focuses on enabling robots to navigate in environments using visual observations and semantic information.
In this context, semantic information refers to the understanding of the environment's objects and their relationships, which can be used to guide the robot's navigation decisions.
\acrshort{vsn} algorithms can be categorized into different approaches based on their training methods and architectures.
For the sake of clarity, this document follows the same categorization as~\cite{gervet2022}, depicted in figure~\ref{fig:vsn-categories}.
This classification separates the approaches into three main categories: classical methods, modular-learning based methods and end-to-end learning methods.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/methods_overview}
    \caption{When navigating via visual observations, several approaches can be taken.
    Classical methods rely on geometrical approaches to map the environment and explore it.
    Modular-learning methods use different modules to decompose the navigation problem into smaller tasks, such as exploration and navigation.
    End-to-end learning methods use deep neural networks to learn the navigation policy directly from visual observations.
    Figure from \cite{gervet2022}.}
    \label{fig:vsn-categories}
\end{figure}

\subsection{Classical methods}\label{subsec:classical-methods}

To navigate in unfamiliar environments, traditional methods use depth sensors~\cite{newcombe2011, thrun2001} and RGB cameras~\cite{jones2011, sattler2018} to build geometric maps and simultaneously determine the robot's position in relation to the map.
This is known as Simultaneous Localization and Mapping (SLAM)~\cite{Kazerouni2022, campos2021, labbe2022}).
Typically, these SLAM models use heuristic algorithms to create graph-based representations of the environment, allowing the robot to visit the different nodes of the graph when navigating to specific points.
Semantic SLAM (\eg~\cite{zhang2018, jin2023}) expands upon SLAM by integrating semantic data from the environment, allowing the robot to identify and store objects in memory.
One of the most well-known approaches in this category is the Kimera system~\cite{rosinol2020} (see figure \ref{fig:kimera-chart}), which uses a modular architecture to combine different modules for visual odometry, semantic segmentation, and object detection.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/kimera_chart_25 Large}
    \caption{Kimera method is one of the most representative in the semantic SLAM category.
    It uses a modular architecture to combine different modules for images and IMU data, outputting pose estimations (a) and multiple metric-semantic reconstructions (b-e).
    Figure from~\cite{rosinol2020}.}
    \label{fig:kimera-chart}
\end{figure}

\subsection{Modular-learning based methods}\label{subsec:modular-learning-based-methods}

Modular-learning based approaches~\cite{chaplot2020, chang2020, skillfusion, Li2023RDDRLAR, zhou2022improving, Cai2024DGMemLV, Wang2023ProbableOL, Wasserman2023ExploitationGuidedEF, Yokoyama2023VLFMVF} decompose the navigation process in separate modules that execute different tasks.
It is common for these methods to be composed of a high-level semantic exploration module trained by RL that indicates the agent subgoals that have to be reached by a low-level navigation policy.
One of the best examples of modularity in \acrshort{vsn} is the Hierarchical Scene Priors NAVigation (HSPNav)~\cite{Kang2024HSPNavHS}, which is shown in figure~\ref{fig:modular-learning}.
HSPNav is a highly modular approach that uses Local and Global Scene Priors to build a semantic understanging of the environment, which then is used by a DQN-based~\cite{mnih2013} navigation policy to navigate to the specified goal.
Modular learning can be also combined with offline RL~\cite{shah2022} techniques to leverage navigation behaviors from fixed datasets, without any additional online data collection or fine-tuning.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/modular_learning}
    \caption{\textbf{Hierarchical Scene Priors NAVigation (HSPNav)} for \acrfull{vsn}.
    This modular-learning based approach decomposes the navigation learning (a) into several modules:
    A semantic mapping module is used to build a semantic map of the environment, which then is further processed by an Local Scene Prior (LSP) module to refine semantically a local map.
    A topological global graph is built at the same time as a Global Scene Prior (GSP).
    This both modules are then fed into a DQN-based navigation policy that outputs the next action to be executed by the robot.
    It also includes a module built on top of RPC (b) to bridge the gap between Habitat simulator and ROS, allowing the use of the Habitat trained policies to be refined on Gazebo simulator.
    Figure from~\cite{Kang2024HSPNavHS}.}
    \label{fig:modular-learning}
\end{figure}

\subsection{End-to-end learning methods}\label{subsec:end-to-end-learning-methods}

A recent approach, made possible by advances in machine learning and computer vision, involves designing navigation policies that directly train deep neural networks to learn semantic information from visual observations in an end-to-end fashion (\eg~\cite{ramrakhya2022, yadav2022, gutierrez2019, khandelwal2022, chaplot2020,chang2020}).
This approach is termed \textit{visual semantic navigation} (\acrshort{vsn}).
These models often rely on the use of CNNs as visual encoders followed by RNNs; that are in charge of predicting an action distribution directly from raw input observations.
The neural networks are trained using imitation learning (IL) or reinforcement learning (RL) approaches.

When IL is applied to the visual navigation problem, navigation policies are learnt from expert demonstrations (\eg~\cite{ramrakhya2022,yadav2022}).
It can also be used combined with an RL fine-tuning phase to achieve better performance~\cite{ramrakhya2023}.

Other works focus on the use of an end-to-end RL approach to solve \objnav navigation~\cite{zhu2017, gutierrez2019, khandelwal2022, Liu2022, Yadav2023OVRLV2AS, Xu2024DeepRL, YokoyamaHM3DOVONAD}.
One of the most notable examples is the DD-PPO~\cite{wijmans2020}, which laid the foundation and framework for training end-to-end and modular learning agents within Habitat simulator.
DD-PPO uses a Proximal Policy Optimization (PPO)~\cite{schulman2017} algorithm to train a visual navigation policy that can navigate to specified goals in a simulated environment.
It also does it via Synchronous-\acrshort{rl} (see figure \ref{fig:ddppo}), which allows the agent to learn from multiple GPU batched environments, speeding up the training process.
Some authors have proposed combining the RL training with different strategies, like auxiliary tasks~\cite{ye2021}, improved visual representations via object relation graphs~\cite{yang2018}, semantic segmentations~\cite{Mousavian2018} or combining audio feedback with the visual inputs~\cite{Wang2023, Kondoh2023MultigoalAN}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/ddppo}
    \caption{\textbf{Decentralized Distributed Proximal Policy Optimization (DD-PPO)}.
    This algorithm is included in the Habitat Lab as a baseline for training end-to-end visual navigation policies.
    It laid the foundation for the majority of the subsequent works in the field, and it is used in this thesis as well.
    While typical \acrshort{rl} have been trained in an asynchronous manner, DD-PPO uses a synchronous approach to train the agent in multiple environments at the same time.
    Figure from~\cite{wijmans2020}.}
    \label{fig:ddppo}
\end{figure}

Finally, there are different approaches that try to tackle the problem of rapidly adapting to unseen environments in visual navigation via meta-learning~\cite{wortsman2019, luo2021, zhang2022}.
These methods are trained on a variety of different environments (usually designated as tasks) and are able to generalize to unseen environments by learning a policy that can be quickly adapted to new environments.
And the recent progress in large language models (LLMs) has led to the possibility of using them to solve the visual navigation problem~\cite{Huang2023, Zhou2023} as well.
In this case, the LLMs are used as a reasoning module in charge of understanding the semantic information present on the environment.
They then share this information with different modules in charge of navigating to the specified goal.

\section{Sparsity and exploration methods}\label{sec:sparsity-and-exploration-methods}
To address the sparse reward and exploration problems, different approaches have been proposed.
Auxiliary tasks~\cite{jaderberg2016, ye2021} help the agent to explore the environment and gather extrinsic reward by maximizing pseudo-reward functions.
Curiosity-driven exploration~\cite{pathak2017} (depicted on figure \ref{fig:curiosity}) leverages on the error of the agent's ability to predict the next state to introduce a new intrinsic reward that enables the agent to explore the environment.
When dealing with procedurally-generated environments, a curriculum learning mechanism can be incorporated so the episodes are ordered by an exploration score~\cite{zha2020b}, and then the agent imitates the best ones.
This thesis also uses procedurally-generated environments, but it relies on a RL approach combined with reward shaping~\cite{ng1999, jestel2021} and $\epsilon\text{-}greedy$~\cite{mnih2013} techniques to learn to navigate in them.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/related_work/curiosity}
    \caption{\textbf{Curiosity-driven exploration} is a technique that uses the error of the agent's ability to predict the next state to introduce a new intrinsic reward that enables the agent to explore the environment.
    The agent learns a forward model that predicts the next state given the current state and action.
    The intrinsic reward is then computed as the error between the predicted and actual next state.
    Figure from~\cite{pathak2017}.}
    \label{fig:curiosity}
\end{figure}

\section{Simulation-to-reality transfer in robotic navigation}\label{sec:simulation-to-reality-transfer-in-robotic-navigation}
Deploying a model trained in simulation to a real robot is a challenging task.
Due to logistical constraints, training a model in the real world —especially with RL techniques— is often impractical, prompting the use of alternative methods to address this challenge.
For example,~\cite{kim2022} proposes a monocular vision-based time-to-collision estimation for small drones by domain adaptation of simulated images.
Their method converts simulated images into real-like synthetic images using a sim-to-real method.
This is done with the aim of minimizing efforts and time invested in the collection of training datasets within real-world scenarios, while simultaneously maximizing the advantages inherent in simulated environments.

Overall, it is necessary to develop methods that allow to efficiently transfer the knowledge learnt in simulation to the real world.
Different approaches have been proposed to solve this problem.
In ~\cite{kadian2020}, an exact replica of a real scenario is simulated (see figure~\ref{fig:sim2real}).
This allows the authors to explore the behavior of visual navigation policies both in simulation and in reality and how their performance correlates.
For instance, CAD2RL~\cite{sadeghiCAD2RLRealSingleImage2017} system achieved remarkable success in training a collision avoidance policy entirely within a simulated environment.
This breakthrough was subsequently tested on real aerial drones, with promising results.
By focusing on simulation refinement~\cite{Son2020}, the accuracy of simulations can be improved by exploiting the disparities between simulated and real-world observations.
In the field of locomotion, training legged robotic systems in a simulated environment and subsequently transferring the acquired policies to real-world applications~\cite{Hwangbo_2019, agarwal2022} has always been a challenging task.

\begin{figure}
    \includegraphics[width=\textwidth]{figures/related_work/sim2real}
    \caption{\textbf{Simulation-to-reality transfer} is a challenging task in robotic navigation.
    In this example, a real-world scenario is simulated to explore the behavior of visual navigation policies both in simulation and in reality.
    The authors use a simulated environment that closely resembles the real-world scenario, allowing them to train and evaluate their policies in a controlled setting.
    Figure from~\cite{kadian2020}.}
    \label{fig:sim2real}
\end{figure}

For the problem of \acrshort{vsn}, the study by~\cite{gervet2022} shows how different approaches behave in real-world settings.
They compare the performance of different \acrshort{vsn} methods in simulated and real-world environments, concluding that modular-learning approaches outperform end-to-end learning methods in real-world scenarios.
Interestingly, the conclusion of chapter~\ref{ch:ros4vsn:-enable-real-world-robotic-visual-semantic-navigation}, is the same as in~\cite{gervet2022}: that modular-learning approaches are the best ones for deployment in the real world.

\section{Offline Reinforcement Learning}\label{sec:offline-reinforcement-learning}

Offline reinforcement learning (offline RL) refers to learning a policy from a fixed dataset of environment interactions, without additional online exploration~\cite{levine2020} (figure \ref{fig:diagram-offline}).
This paradigm is appealing for robotics, where collecting data is costly or risky, since it allows agents to leverage large logs of prior experiments or demonstrations.
However, a core challenge in offline RL is distributional shift: the learned policy may choose state-action pairs not well-covered by the dataset, leading to arbitrarily bad value estimates and unstable policy updates.
Over the past years, a number of algorithmic strategies have been proposed to address this issue and enable effective offline learning of robotic skills.

\begin{figure}
    \includegraphics[width=\textwidth]{figures/related_work/diagram_offline}
    \caption{\textbf{Offline reinforcement learning} is a paradigm that allows training models using fixed datasets, without the need for online data collection.
    In this diagram, the offline RL process is depicted, where a dataset of environment interactions is used to train a policy.
    The policy is then evaluated in the environment, and the process is repeated until convergence.
    Figure from~\cite{levine2020}.}
    \label{fig:diagram-offline}
\end{figure}

\subsection{Conservative Value Estimation}\label{subsec:conservative-value-estimation}

One line of work imposes \textit{pessimism} in value function learning to avoid overestimating unseen actions.
For example, Conservative Q-Learning (CQL)~\cite{conservative} augments the Bellman objective with a penalty on Q-values for out-of-distribution actions.
By learning a deliberately conservative Q-function (lower-bounding the true values), CQL prevents the policy from exploiting spurious high rewards for actions outside the dataset support.
Similarly, Implicit Q-Learning (IQL)~\cite{kostrikov2022offline} avoids querying unseen actions altogether during training.
Instead of a max over actions, IQL uses an expectile regression to estimate the value of the \textit{best} in-dataset actions, then extracts a policy via advantage-weighted behavior cloning.

\subsection{Policy Constraints and Behavior Regularization}\label{subsec:policy-constraints-and-behavior-regularization}

Another broad strategy is to constrain the learned policy to stay close to the behavior policy that generated the data.
A prominent example is Batch-Constrained deep Q-learning (BCQ)~\cite{Fujimoto2018OffPolicyDR}, which modifies the Q-learning update to consider only actions that are likely under the offline dataset instead of maximizing over all actions.
In practice, BCQ trains a generative model (e.g. a VAE) to produce candidate actions similar to those in the dataset, ensuring that the policy cannot stray far from logged behavior.
Likewise, Bootstrapping Error Accumulation Reduction (BEAR)~\cite{Kumar2019StabilizingOQ} and Behavior-Regularized Actor-Critic (BRAC)~\cite{Wu2019BehaviorRO} add explicit divergence penalties (e.g. KL or MMD) to keep the learned policy distribution close to the dataset behavior policy.
By limiting policy deviation, these methods address bootstrapping error and distribution shift, effectively preventing the agent from choosing out-of-distribution actions with erroneously high value estimates.
Variants of this idea combine RL updates with imitation learning: for instance, Advantage-Weighted Actor-Critic (AWAC)~\cite{Nair2020AcceleratingOR} and Critic-regularized Regression (CRR)~\cite{NEURIPS2020_588cb956} constrain the policy toward the dataset behavior while weighting actions by their estimated advantages, thus improving performance without sacrificing stability.
Overall, policy constraint and behavior regularization approaches have proven effective both in simulation and in real-world settings by leveraging the data’s demonstrated behaviors as an anchor.

\section{Meta Reinforcement Learning}\label{sec:meta-reinforcement-learning}