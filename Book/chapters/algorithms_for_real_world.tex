\chapter{Beyond Reinforcement Learning for Real World Robotic Visual Navigation}\label{ch:rl4rvsn}

When it comes to training robots to navigate in the real world, reinforcement learning (RL) has been the go-to approach for many years.
However, RL has its limitations, especially when it comes to real-world applications.
Using online reinforcement learning (RL) algorithms require querying environments to learn.
This is a problem because querying real environments is expensive and time-consuming, and querying simulated environments is not always a good proxy for real-world performance.

Furthermore, another limitation of online RL is that it requires a large number of interactions to learn.
This is known as sample-inefficiency, and it is a major bottleneck for real-world applications, as it requires a lot of time and resources to collect enough interactions to train a robot.
In this chapter, we explore two alternative approaches to RL for robotic visual navigation: Offline Reinforcement Learning and Meta Imitation Learning.


\section{Offline Reinforcement Learning for Robotic Visual Navigation}\label{sec:offline_rl4rvsn}

\subsection{Introduction}\label{subsec:introduction_offnav}

The first approach that we explore is Offline RL~\cite{levine2020}.
Offline RL consist on learning policies from a fixed dataset consisting in human demonstrations and their associated reward signals.
This can be a powerful approach for training agents in complex environments, as it allows the agent to learn from a large amount of data without the need to interact with the environment.
Therefore, in this work, we propose a novel approach to train \acrshort{vsn} agents without ever querying an environment, by leveraging on the Offline RL paradigm.
We call this approach \textbf{Off}line Visual Semantic \textbf{Nav}igation (OffNav).

Technically, we have implemented Implicit Q-Learning (IQL)~\cite{kostrikov2022offline} offline RL algorithm using the decentralized distributed philosophy of DD-PPO~\cite{wijmans2020} to create DD-IQL, a decentralized distributed version of IQL\@.
Our DD-IQL is trained against a fixed dataset containing thousands of human navigation experiences~\cite{ramrakhya2023}.
As depicted in Figure~\ref{fig:abstract_offnav}, we propose the OffNav approach, capable of efficiently learning the navigation policy required by a \acrshort{vsn} agent from human demonstrations.
Subsequently, these policies can be deployed across various scenarios, and if necessary, further refined through online RL for more specific tasks.

To demonstrate the capabilities of our implementation, we carried out a small analysis of its performance using different environments from HM3D dataset~\cite{Ramakrishnan2021HabitatMatterport3D}.
Preliminary results shows that our DD-IQL implementation is able to learn navigation policies effectively.
To the best of our knowledge, this is the first time that an offline RL algorithm is implemented for \acrshort{vsn}\@ and large environments, predicting actions directly from raw input observations.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/offnav/graphical_abstract}
    \caption{
        By leveraging on the offline reinforcement learning paradigm, we can train agents from a fixed dataset of navigation experience, without querying any environment.
        This opens the possibility to create many navigation datasets from any navigation agent in any \textbf{real or simulated} environment, and then use them to train new agents for different scenarios without the need to ever query that environment.
    }
    \label{fig:abstract_offnav}
\end{figure}

\subsection{Offline Visual Semantic Navigation}\label{subsec:offline-navigation}

In this work, we study \acrshort{objnav} navigation~\cite{batra2020}, a setup in which an agent is asked to navigate to a target object in an environment.
To perform this task, the agent does it using only egocentric perceptions.
Specifically, the agent receives RGB images and GPS+Compass information that provides the agent with the current position and orientation relative to the starting point.
The set of movements is discrete and consists of the following actions: \turnleft, \turnright, \moveforward, \lookup, \lookdown and \stopac.
If the agent samples the \stopac action within 1m Euclidean distance respect to the target object within a 500 steps time limit, the episode is considered successful.
In the other case, it is considered a failure.
The performance of the navigation is measured by averaging the success over all the episodes present in an evaluation, and it receives the name of Success Rate (SR).
We also report the Success weighted by Path Length (SPL) metric, which is the success rate weighted by the ratio between ideal and actual path length.

Since we are on an offline RL setup, we need a previously collected dataset of navigation experience.
The dataset that we chose is collected in~\cite{ramrakhya2023}.
It consists of 77k episodes of human navigation trajectories using the HM3D~\cite{Ramakrishnan2021HabitatMatterport3D} dataset.

We train our policies using our DD-IQL implementation on the human demonstrations.
The objetive is to find a policy with optimal parameters $\phi^*$ that maximizes the expected return from the dataset.
To do so, the IQL algorithm relies on the use of expectile regression to modify a temporal-difference (TD) loss.
This modified TD loss is able to learn an approximate Q-function from the dataset actions.
This Q-function does not explicitly represent the corresponding policy, so a separate policy extraction step is needed.
For policy extraction, we use advantage-weighted regression~\cite{peters2007, peng2019advantageweighted}:

\begin{equation}
    L_\pi(\phi)=\mathbb{E}_{(s, a) \sim \mathcal{D}}\left[\exp \left(\beta\left(Q_{\hat{\theta}}(s, a)-V_\psi(s)\right)\right) \log \pi_\phi(a|s)\right]\; ,
    \label{eq:loss}
\end{equation}

where $\beta \in [0, \infty)$ controls the trade-off between cloning the expert policy and maximizing the Q-function.
This loss can be seen as a selection of most optimal actions to clone in the dataset.
We also employ inflection weighting~\cite{wijmans2019} to modify the loss function, thereby giving more importance to those time steps where there is a change in actions.

For the policy architecture, we use a simple CNN+RNN model from\cite{ramrakhya2023}.
The difference is that we use ResNet18 for the visual encoders.
We copy the same architecture for the policy net, the Q net and the Q target net.
For the V net, we only use the visual encoder and a single linear layer, without any recurrent module.

\subsection{Experiments and Results}\label{subsec:experiments_offnav}

Is an offline RL algorithm able to learn navigation policies effectively?
To answer this question, we have trained our DD-IQL model using the expert demonstrations on five different experimental setups.
These setups have been designed with an incremental difficulty.
The first three are evaluated on the same environments in which the agents were trained, while the last two are evaluated on different environments.
The details of the setups are depicted on figure~\ref{fig:setups}.

We compare our results with the current state-of-the-art model PirlNav~\cite{ramrakhya2023}.
This model is based on a two-phase training schedule.
The first phase is a supervised learning phase, where the model is trained using behavior cloning on the expert demonstrations.
The second phase is a reinforcement learning phase, where the model is fine-tuned using DD-PPO~algorithm~\cite{wijmans2020}.
For a fair comparison, we train the PirlNav agent using only the behavior cloning phase on the same setups as our OffNav model.

Results are shown on table~\ref{tab:success}.
It can be seen that both methods obtain similar performance on setups 1 to 3.
Offnav method outperforms PirlNav on setup 2, while PirlNav outperforms OffNav on setup 3, and both of them obtain 100\% SR on setup 1.
When evaluated on setup 4, PirlNav outperforms OffNav by 2.27\% absolute points.
However, on setup 5, the most challenging one, OffNav outperforms PirlNav by 8.69\% absolute points.

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        \textit{Experimental Setup} & \textit{OffNav}  & \textit{PirlNav} \\
        \midrule
        \textsc{Setup 1}            & 100\%            & 100\%            \\
        \textsc{Setup 2}            & \textbf{79.31\%} & 72.50\%          \\
        \textsc{Setup 3}            & 75.78\%          & \textbf{77.63\%} \\
        \textsc{Setup 4}            & 25.00\%          & \textbf{27.27\%} \\
        \textsc{Setup 5}            & \textbf{34.78\%} & 26.09\%          \\
        \bottomrule
    \end{tabular}
    \caption{Success Rate for OffNav and PirlNav methods on the five experimental setups.}
    \label{tab:success}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/offnav/experimental_setups}
    \caption{Five experimental setups designed with an incremental difficulty.}
    \label{fig:setups}
\end{figure}

\subsection{Conclusions and Future Work}\label{subsec:conclusions_offnav}

From the results obtained in the experiments, we can conclude that the proposed OffNav method is able to learn navigation policies effectively from human demonstrations.
It can also be seen that the method is able to generalize to unseen environments, as shown in setups 4 and 5, and outperform the state-of-the-art model PirlNav~\cite{ramrakhya2023} in the most challenging one.
Future work will focus on training the policy with more diverse environments to improve its generalization capabilities and further extend this analysis.


\section{Meta Imitation Learning for Real World Navigation}\label{sec:mil-for-real-world-navigation}

\subsection{Introduction}\label{subsec:introduction_metanav}

In section~\ref{sec:offline_rl4rvsn}, we explored the possibility of training agents from a fixed dataset of human demonstrations using offline reinforcement learning.
Despite the elimination of the need to query environments, this approach still requires a large amount of data to learn effectively.
This reduces the applicability of this approach in real-world scenarios, where collecting large datasets can become an unfeasible task.
To address this issue, we explore a different approach that can also learn from a fixed dataset, but with the ability to adapt quickly to new tasks with few examples.
This will help us to bridge the gab between training agents in simulation and deploying them in the real world.

This second approach that we explore is known as Meta Imitation Learning~\cite{finnOneShotVisualImitation2017}.
Here we enter into a combination of two different paradigms: Imitation Learning and Meta Reinforcement Learning.
On the one hand, Meta Reinforcement learning~\cite{Beck_2025} is a set of techniques that try to teach agents to learn how to learn, enabling them to adapt quickly to new tasks with few examples.
Imitation Learning~\cite{10602544}, on the other hand, is a paradigm that allows agents to learn from demonstrations provided by an expert.
Thus, Meta Imitation Learning (MIL) is a combination of both realms, allowing agents to learn from a small number of demonstrations and adapt quickly to new tasks.
We could describe our approach as a meta-learning algorithm that learns to imitation learn.
Figure~\ref{fig:abstract_metanav} illustrates the proposed approach, which we call \textbf{Meta} Visual Semantic \textbf{Nav}igation (MetaNav).
In our setting, we learn a parametrized policy $\pi_\phi$ that can adapt to new tasks by learning from a small number of gradient updates.
Due to this adaptability, we can train the policy on simulation using a set of task demonstrations, and then deploy it in the real world to perform new tasks.

The main question that we want to answer is: \textit{Can we learn visual navigation policies via meta imitation learning that can adapt to new tasks with few examples?}
To answer this question, we have implemented a meta imitation learning algorithm based on the work of~\cite{finnOneShotVisualImitation2017}.
We train our algorithm on a small set of different environments from HM3D dataset~\cite{Ramakrishnan2021HabitatMatterport3D}.
Preliminary results show that our algorithm is able to learn visual navigation policies that can adapt to new tasks with few examples.
To the best of our knowledge, this is the first time that a meta imitation learning algorithm is implemented for \acrshort{vsn}\@ and large environments, predicting actions directly from raw input observations.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/metanav/graphical_abstract}
    \caption{
        By leveraging on the meta imitation learning paradigm, we can train agents to adapt to new tasks with few examples.
        This opens the possibility to create many navigation datasets from any navigation agent in any \textbf{real or simulated} environment, and then use them to train new agents for different scenarios without the need to ever query that environment.
    }
    \label{fig:abstract_metanav}
\end{figure}

\subsection{Meta Imitation Learning for Robotic Visual Navigation}\label{subsec:meta-imitation-learning-for-robotic-visual-navigation}

In this section, we describe the proposed Meta Imitation Learning algorithm for robotic visual navigation.
We follow the approach of~\cite{finnOneShotVisualImitation2017}, in which a vision-based policy is trained to adapt to new tasks with few examples.

\subsubsection{Problem setting}\label{subsubsec:problem-setting}

As described in section~\ref{subsec:offline-navigation}, we focus on \acrshort{objnav} navigation~\cite{batra2020}, where an agent must reach a specified target object within an environment by sampling actions from a discrete set.
The available actions are discrete and include: \turnleft, \turnright, \moveforward, \lookup, \lookdown, and \stopac.
An episode is considered successful if the agent selects the \stopac action within 1 meter of the target object and within a 500-step limit; otherwise, it is deemed a failure.
The agent relies solely on egocentric observations to accomplish this task.
In particular, it receives RGB images and GPS+Compass data, which provide its current position and orientation relative to the starting point.

Since we are on an imitation learning setup, we need a previously collected dataset of navigation experience.
The dataset that we chose is collected in~\cite{ramrakhya2023}.
It consists of 77k episodes of human navigation trajectories using the HM3D~\cite{Ramakrishnan2021HabitatMatterport3D} dataset.
This dataset consists of a set of photorealistic 3D indoor environments, each containing a variety of objects and different scenes.

Finally, we define a task as a tuple of the form $\mathcal{T} = (G, S)$, where $G$ is the target object and $S$ the scene in which the navigation is taking place.
Each task can contain multiple demonstrations $\tau_i$, each of them recorded by a human and always on the same scene and seeking the same target object.
The variability of the demonstrations comes from the random starting points in each episode and the different paths that the human can take to reach any of the several instances of the target object.
Thus, our goal is to learn a policy $\pi_\theta$ that can quickly adapt to combinations of new target objects $G$ and scenes $S$.

\subsubsection{Problem formulation}\label{subsubsec:problem-formulation}

We consider a set of tasks $\mathcal{T}_i = (G, S)$ drawn from a distribution $p(\mathcal{T})$.
Each task $\mathcal{T}_i$ consists of a collection of observations $o$ and actions $a$ generated by an expert policy $\pi^*_i$:
\begin{equation}
    \tau = \{o_1, a_1, \dots, o_T, a_T\} \sim \pi^*_i,
\end{equation}

where $T$ is the length of the demonstration or trajectory $\tau$.
Our objective is to learn a policy $\pi_\theta$ that maps observations $o$ to predicted actions $\hat{a}$ from different demonstrations sampled from the task distribution.
Since we are in an imitation learning context with discrete actions, we use behavior cloning loss to train the policy from the demonstrations:

\begin{equation}
    \mathcal{L}^*_{\mathcal{T}_i}(\pi_\theta) = \sum_{\tau_j \sim \mathcal{T}_i} \sum_{(o_t, a_t) \in \tau^i}^T -\log (\pi_\theta(a_t|o_T)).
    \label{eq:loss_metanav}
\end{equation}

This loss function is known as the inner loss, and its function is to adapt the policy to a specific task.
By applying gradient descent to the inner loss, we can compute the adapted parameters $\theta^\prime_i = \theta-\alpha\nabla_{theta}\mathcal{L}^*_{\mathcal{T}_i}(\pi_\theta)$ of the policy.
Inflection weighting~\cite{wijmans2019b} is applied to the inner loss function to emphasize time steps corresponding to action changes.

Once we have the adapted parameters $\theta^\prime_i$, we can use them to compute the outer loss, which is used to update the policy parameters $\theta$.
The outer loss is also known as the meta-loss or meta-objective, and it is defined as:

\begin{equation}
    \min _{\theta} \sum_{\mathcal{T}_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}\left(\pi_{\theta_i^{\prime}}\right)=\sum_{\mathcal{T}_i \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_i}\left(\pi_{\theta-\alpha \nabla_\theta \mathcal{L}_{\mathcal{T}_i}^*\left(\pi_\theta\right)}\right).
    \label{eq:meta_loss_metanav}
\end{equation}

Intuitively, this meta-loss acts as a regularization term over a ``fine-tuning'' process over the tasks.
This encourages the policy to find an average over $\theta$ meta-parameters with respect to the task distribution.
Then, the meta-parameters can adapt to new tasks with few gradient updates, since the distance between the adapted parameters $\theta^\prime_i$ and the original parameters $\theta$ is being minimized across set of tasks.
Note that for this goal, the meta-loss implies the use of double derivatives, which can be computationally expensive.
In practice, we can use a first-order approximation of the meta-loss~\cite{finn2017}, which allows us to compute the outer loss without the need for second derivatives.

\subsection{Experiments}\label{subsec:experiments_metanav}

\subsubsection{Experimental setup}\label{subsubsec:experimental-setup}

We use the same dataset divided in five setups as in section~\ref{subsec:experiments_offnav}.
This dataset is a reduced version of the one used in~\cite{ramrakhya2023}, so the algorithm can be trained in a reasonable amount of time.
A scheme with the details of the setups is depicted in figure~\ref{fig:setups}.
However, in this case, the setups four and five do not include human demonstrations.
In that case, they had to be generated by a different pretrained model.
The one chose was the PirlNav model~\cite{ramrakhya2023}, which has 70.4\% SR on the whole HM3D evaluation split.

The policy architecture is based on a straightforward CNN+RNN model as described in~\cite{ramrakhya2023}, with the modification that ResNet18 is employed for the visual encoders.
The implementation of the algorithm is based on the decentralized distributed philosophy of DD-PPO~\cite{wijmans2020}, which allows us to train the algorithm in a distributed manner.
We adapt the DD-PPO algorithm to the meta imitation learning setting by modifying the inner loss function to include the behavior cloning loss from equation~\ref{eq:loss_metanav} and include the outer loss function from equation~\ref{eq:meta_loss_metanav}.

\subsubsection{Evaluation}\label{subsubsec:evaluation_metanav}

Since the algorithm is trained on a meta-learning setting, it cannot be evaluated the same way as the rest of the navigation models have been evaluated throughout this book.
In the meta-evaluation, the algorithm is presented with a set of new tasks that has never seen during training.
It first adapts the policy to the task using experience from the task demonstrations to obtain the adapted parameters $\theta^\prime_i$.
Then, it evaluates the adapted policy on the task by sampling actions from the adapted policy and measuring the performance of the agent.

The evaluation can be done in two different ways: continuous evaluation and per episode evaluation.
In continuous evaluation, the agent receives a fixed number of steps to adapt to the task, and then it is evaluated on the same task for a fixed number of steps.
This is done continuously for all the episodes present in the task and all the tasks present in the evaluation.
In per episode evaluation, the agent receives a fixed number of steps to adapt to the task.
Then it is evaluated on the same task until the episode is finished, either by a success or by a reach of the max steps.
The agent always uses 64 steps for adaptation, and in continuous evaluation, it uses 64 steps for evaluation.
Navigation performance is evaluated again by Success Rate (SR) and Success weighted by Path Length (SPL) metrics.
Additionally, we also report the average distance to the goal at the end of the episode, which is known as Distance to Goal.

\subsection{Results}\label{subsec:results_metanav}

The first goal is to compare how the algorithm performs under the two different evaluation methods.
The results of the continuous evaluation are shown in table~\ref{tab:metanav_continuos}, while the results of the per episode evaluation are shown in table~\ref{tab:metanav_episode}.
It can be seen that the results vary between setups, but the continuous evaluation gives better results on all the setups and metrics except for setup five.
The main reason for this lies in the fact that the agent was trained also in a continuous fashion, so it is more adapted to this type of evaluation.
Another reason is that the continuous evaluation presents more experience to the agent per episode compared to the per episode evaluation.
In the per episode evaluation, the agent only has 64 steps to adapt to the task, while in the continuous evaluation, it has 64 steps to adapt and then 64 steps to evaluate.
However, if the episode is not finished, it can continue adapting and evaluating until the episode is finished.

\begin{table}
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textit{\textbf{Setup}} & \textit{\textbf{SR ($\uparrow$)}} & \textbf{\textit{SPL ($\uparrow$)}} & \textit{\textbf{Distance to Goal ($\downarrow$)}} \\ \midrule
        1                       & 89.18\%                           & 40.04\%                            & 0.29                                              \\
        2                       & 76.10\%                           & 33.92\%                            & 0.97                                              \\
        3                       & 64.19\%                           & 33.11\%                            & 1.99                                              \\
        4                       & 23.07\%                           & 11.87\%                            & 12.23                                             \\
        5                       & 21.74\%                           & 9.38\%                             & 7.99                                              \\
    \end{tabular}
    \caption{Evaluation of MetaNav on the \acrshort{vsn} task. Results obtained with continuous evaluation.}
    \label{tab:metanav_continuos}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textit{\textbf{Setup}} & \textit{\textbf{SR ($\uparrow$)}} & \textbf{\textit{SPL ($\uparrow$)}} & \textit{\textbf{Distance to Goal ($\downarrow$)}} \\ \midrule
        1                       & 83.33\%                           & 40.03\%                            & 0.29                                              \\
        2                       & 60.78\%                           & 26.58\%                            & 1.74                                              \\
        3                       & 55.19\%                           & 26.21\%                            & 2.54                                              \\
        4                       & 16.67\%                           & 4.84\%                             & 12.72                                             \\
        5                       & 25.00\%                           & 9.31\%                             & 8.19                                              \\
    \end{tabular}
    \caption{Evaluation of MetaNav on the \acrshort{vsn} task. Results obtained with per episode evaluation.}
    \label{tab:metanav_episode}
\end{table}

The next goal is to compare the performance of the algorithm with the newest models.
We compare our results with state-of-the-art model PirlNav~\cite{ramrakhya2023} and the OffNav model from section~\ref{sec:offline_rl4rvsn}.
The results of the comparison are shown in table~\ref{tab:metanav_comparison}.
It can be seen that in almost any of the setups, the MetaNav model outperforms the OffNav or PirlNav models.
Only in setup 2, MetaNav is superior to PirlNav, while OffNav outperforms both of them.

\begin{table}
    \centering
    \begin{tabular}{c|ccc}
        \toprule
        \textit{Experimental Setup} & \textit{OffNav}  & \textit{PirlNav} & \textit{MetaNav} \\
        \midrule
        \textsc{Setup 1}            & \textbf{100}\%   & \textbf{100}\%   & 89.18\%          \\
        \textsc{Setup 2}            & \textbf{79.31\%} & 72.50\%          & 76.10\%          \\
        \textsc{Setup 3}            & 75.78\%          & \textbf{77.63\%} & 64.19\%          \\
        \textsc{Setup 4}            & 25.00\%          & \textbf{27.27\%} & 23.07\%          \\
        \textsc{Setup 5}            & \textbf{34.78\%} & 26.09\%          & 25.00\%          \\
        \bottomrule
    \end{tabular}
    \caption{Success Rate for the three models on the different experimental setups.}
    \label{tab:metanav_comparison}
\end{table}

\subsection{Conclusions}\label{subsec:conclusions_metanav}

The experimental results demonstrate that the proposed MetaNav approach can effectively learn visual navigation policies capable of adapting to new tasks with limited examples.
However, the performance of the algorithm varies across different setups, and it is not able to outperform the state-of-the-art models in all cases.
This is a contradictory result, as the model is given a little additional experience from the evaluation episodes, while the other models do not have any input from the evaluation episodes.
This could be due to the meta imitation learning setup, which is based on a policy-gradient method~\cite{Beck_2025}.
Further work is needed to search for different meta-learning algorithms that can improve the performance of the algorithm, without changing the main training philosophy of the algorithm.

\section{Training Problems}\label{sec:training-problems}

Despite presenting totally different approaches to tackle the problem of robotic visual navigation on this chapter, both Offline Reinforcement Learning and Meta Imitation Learning have suffered from the same limitation.
This limitation is its inability to learn navigation policies from the whole set of training environments present in the HM3D dataset~\cite{Ramakrishnan2021HabitatMatterport3D}.
This is the reason why all the experiments presented in this chapter have been done using a reduced version of the HM3D dataset.

% Hablar de que por eso hemos usado un dataset reducido.
% Poner un par de gráficas de entrenamientos para ver que la pérdida no se estabiliza.
In figure~\ref{fig:training_problems}, we can see the training curves of both algorithms on the full training dataset.
The training curves show that the loss does not stabilize, and the performance of the algorithms does not improve over time.
This is a clear indication that the algorithms are not able to learn from the whole dataset, and they are not able to generalize to new environments.

% Create a two-column figure with the training curves of both algorithms.
\begin{figure}
    \centering
    \includegraphics[width=0.49\linewidth]{figures/metanav/offnav_losses_avg_std}
    \includegraphics[width=0.49\linewidth]{figures/metanav/metanav_losses_avg_std}
    \caption{Training curves of OffNav (left) and MetaNav (right) on the full training dataset. The loss does not stabilize, and the performance of the algorithms does not improve over time.}
    \label{fig:training_problems}
\end{figure}

% Hablar filosoficamente de que parece que el problema procede de que estos algoritmos alternativos a RL no son capaces de aprender de un dataset tan grande, ya que el modelo funciona bien bajo otros algoritmos más sencillos.
% Mencionar el tema de que hay aproximaciones más promising como por ejemplo task inference methods, que no modifican sustancialmente el algoritmo subyacente, pero que permiten al algoritmo aprender de un dataset más grande.