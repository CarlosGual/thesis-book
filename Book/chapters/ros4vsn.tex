\chapter{Enableling Real World Robotic Visual Semantic Navigation}\label{ch:ros4vsn:-enable-real-world-robotic-visual-semantic-navigation}

%Motivation an \acrshort{vsn} problem description.
\lettrine{\textcolor{accent_color}{C}}{an} a robotic agent navigate and interact in the real world as seamlessly as humans do?
This is the fundamental question driving research within the embodied \acrshort{ai} community.
The problem is formally known as \acrlong{vsn}, \eg~\cite{ramrakhya2023,Cai2024DGMemLV,chang2020}.
However, mimicking human navigation is a challenging task for robots, particularly in unseen environments, as it requires efficient exploration and a deep understanding of the objects and structures within the space.
For unknown scenarios, humans can leverage prior semantic information achieved from previous scenes to navigate in new environments, but it is still a challenging task to incorporate that knowledge into embodied agents, especially in real robotic platforms navigating in the real world.
The potential of autonomous robots with these advanced navigation capabilities is vast, ranging from assistive robots that can guide individuals with reduced mobility to specific locations, to platforms that can aid in complex environments such as search and rescue operations or logistic centers.

%Problem statement -> \acrshort{vsn} is solved in a virtual world!
Technically, this chapter focuses on embedding \acrshort{vsn} models in real robotic platforms.
This represents the main objective of the work presented.
The focus is on the \acrfull{objnav}~\cite{batra2020} problem.
As observed in figure~\ref{fig:abstract_ros4vsn}, in the \acrshort{objnav} task the agent has to navigate from a random position to certain object goals present in the scene, mainly using vision-based sensors.
In contrast with traditional geometric navigation approaches, where the navigation problem is solved typically by using a map or generating it on the fly (e.g., \archort{slam}), \acrshort{vsn} models are learning-based approaches that use \emph{no} metric map.
Therefore, \acrshort{vsn} solutions must learn visual representations of the environment to reduce the exploration time and better generalize to unseen scenes and object categories.
Notably, many of these solutions combine \acrshort{rl} and/or \acrshort{il} strategies with recent advances in \acrshort{dl} models for visual perception to address the navigation problem in \emph{virtual} environments, where embodied \acrshort{ai} agents are trained and tested.
However, it is crucial to thoroughly investigate how the latest solutions to the \acrshort{vsn} problem perform when deployed with real robots navigating in real-world environments.
This is where this chapter makes its most significant contribution.
Although there are \acrshort{vsn} works that achieve \textit{near-human} navigation performance, \eg~\cite{ramrakhya2023}, these systems are mainly trained and tested in virtual environments, so how these models would behave in the real world remains a question to be answered.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/abstract_paper}
    \caption{
        \acrshort{objnav} task is a complex navigation problem.
        An agent needs to employ vision-based sensors to navigate from a random starting point to specific object goals within the scene.
        Many different hardware and software components need to be fully integrated to solve it, making it difficult to deploy and test these \acrshort{vsn} models in real robots.
        Therefore, the best current solutions are trained and tested in virtual environments.
        The goal is to bridge the gap between virtual and physical environments by providing a ROS-based framework that simplifies testing and comparing various \acrshort{vsn} models on real robotic platforms.
    }
    \label{fig:abstract_ros4vsn}
\end{figure}

This chapter aims to build true embodied agents by proposing a novel solution for the integration of \acrshort{vsn} models into real robots.
The main contributions of this chapter are as follows:
\begin{enumerate}
    \item The first contribution is the provision to the embodied agents community of the ROS4VSN development, a novel \acrfull{ros}~\cite{ros} based architecture that enables testing and comparing different \acrshort{vsn} models in real robots.
    The ROS4VSN development is model agnostic; thus, any \acrshort{vsn} solution can be integrated into it with ease.
    Section~\ref{subsec:ros4vsn} includes all the technical details.
    \item The second contribution involves the embedding of two state-of-the-art \acrshort{vsn} models in two different robotic platforms.
    The selected models are PIRLNav~\cite{ramrakhya2023} and VLV~\cite{chang2020}.
    To achieve this integration, it is necessary to make technical adaptations to the models so that they transition from interacting with observations provided by simulation environments to observations from the real world.
    All these technical modifications to the models are detailed in Section~\ref{subsec:vsn_models}.
    \item Finally, an experimental evaluation of the adapted \acrshort{vsn} models is presented, using the ROS4VSN framework, with two different robots, in a real world scenario (Section~\ref{sec:experiments}).
    The main question addressed with the designed experiments is: Are the state-of-the-art \acrshort{vsn} models able to successfully operate with real robots?
    The success rate in real world navigation experiments has been measured, which has enabled the analysis of the difference in performance compared to those tests in simulation environments, where embodied \acrshort{ai} agents are typically tested.
    This chapter demonstrates that these \acrshort{vsn} solutions perform noticeably differently when evaluated in real-world and simulated situations.
\end{enumerate}


\section{Methodology}\label{sec:methodology}
%Intro
The main objective of this research has been to efficiently integrate various state-of-the-art \acrshort{vsn} models on multiple robotic platforms.
To fulfill this goal, the first step of the proposed methodology has been to develop a \acrshort{ros}-based solution to ease the integration of \acrshort{vsn} solutions into real robots.
It is crucial that the approach is model-agnostic, allowing the integration of any \acrshort{vsn}-designed model.
This development has been named ROS4VSN, and it is detailed in Section~\ref{subsec:ros4vsn}.
Once the ROS4VSN system is available, some state-of-the-art \acrshort{vsn} models need to be selected that will enable the experimental evaluation with real robots.
In Section~\ref{subsec:vsn_models}, the selected \acrshort{vsn} models are justified and the technical modifications made to integrate them into ROS4VSNare detailed.

\subsection{ROS4VSN: ROS for Visual Semantic Navigation}
\label{subsec:ros4vsn}

The ROS4VSN library has been designed to be modular and flexible so that it can be easily adapted to different robots and \acrshort{vsn} models.
It is built on top of \acrshort{ros} \textit{Noetic}~\cite{ros} open-source robotic middleware, because of its flexibility, support, compatibility, and popularity among the robotics community.
\acrshort{ros} provides a collection of useful tools, libraries, and conventions to simplify the task of creating complex and robust robot behaviors across a wide variety of robotic platforms, which makes it suitable for this framework.
The architecture of the framework has been designed to have three main capabilities: it can receive and process information from the environment, infer actions using an \acrshort{ai} \acrshort{vsn} model, and control the actuators of a platform to reach a specific navigation goal.
This design facilitates the integration of different \acrshort{vsn} models, since it only requires replacing the model with which the experiments are to be carried out.
The architecture is divided into the following main packages, each of which plays a specific role: robot\_api, camera\_api, discrete\_move, and visual\_semantic\_navigation.
These packages are connected to each other through \acrshort{ros} topics and services, and also to external hardware devices: a camera (RGB + Depth) and a differential drive robotic platform.
Figure~\ref{fig:arch_scheme} shows a visual representation of the global architecture scheme, illustrating the connections between the different developed \acrshort{ros} packages and the hardware devices.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/arquitectura_paper}
    \caption{Architecture scheme of the ROS44\acrshort{vsn} framework.
    It shows the different packages, topics and connections within them and the hardware devices.}
    \label{fig:arch_scheme}
\end{figure}

\subsubsection{Robot API}\label{subsubsec:robot-api2}

This package is responsible for controlling the actuators of the robot and sending odometry information to the discrete\_move and visual\_semantic\_navi\-gation packages (depicted in figure~\ref{fig:arch_scheme}).
It is typically designed by the manufacturer of the robot, so it can be different depending on the employed platform.
In the particular case of this research, the development of the framework and experiments were done using two robots (see Figure~\ref{fig:robots}).
First, a Turtlebot 2 robot, so the standard turtlebot2~\cite{kobuki} package is integrated as robot\_api.
Since the Turtlebot 2 expects the velocity commands in the \textit{/mobile\_base/commands/velocity} topic, a remapping to the \textit{/cmd\_vel} topic from the discrete\_move package is performed.
The second robot is known as LOLA2~\cite{LOLA}.
A complete robot\_api package has been developed to guarantee the compatibility with the rest of ROS4VSN architecture.

In charge of the communication with the platform, this package is also responsible for publishing the robot's odometry information through the topic \textit{/odom} (odometry topic).
This information is crucial for the visual\_seman\-tic\_navigation and discrete\_move packages.
On one hand, the package discrete\_move uses this information to adjust the velocity commands sent to the robot\_api package, achieving precise and controlled movements.
On the other hand, the visual\_semantic\_navigation package can use the odometry information to help infer the action to be executed by the robot or to help a planner reach its destination.

\subsubsection{Camera API}\label{subsubsec:camera-api}

This package is responsible for capturing RGB and depth images from the camera.
It publishes them through the \textit{/camera/color} and \textit{/camera/depth} topics, respectively.
The camera used in the robotic platforms is the Orbecc Astra S, an RGB plus depth camera, based on structured light technology.
The official ros\_astra\_camera package~\cite{orbeccros} has been adapted to be integrated in the ROS4VSN architecture.
The modular design of ROS4VSN allows it to be used with any other type of camera on the robots, simply by adapting this Camera \acrshort{api} package.

\subsubsection{Discrete Move package}\label{subsubsec:discrete-move-package}

This package has been developed with the purpose of providing precise and customizable control of the robotic platform through discrete navigation commands.
Note that this is the way most \acrshort{vsn} models interact with their agents (\eg~\cite{ramrakhya2023,chang2020}).
In embodied \acrshort{ai}, navigation in simulated environments is performed using discrete action commands that agents execute to reach the specific goals: move forward (25 cm), turn left or right (30 degrees), or stop.
The ROS4VSN package acts as a server to which clients can request a set of discrete movements and configure the forward distance or turning angles.
The package communications scheme is shown in detail figure~\ref{fig:discrete_move}.
It is responsible for executing the actions requested by the visual\_semantic\_navigation package and sending a response when the action is completed.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/comunicaciones_service}
    \caption{Communications between visual\_semantic\_navigation and discrete\_move packages.}
    \label{fig:discrete_move}
\end{figure}

\paragraph*{\textbf{Set of navigation movements}}\label{par:movement-set}

The set of movements allowed by the package consists of the following actions: \turnleft, \turnright, \moveforward, \movebackward, and \stopac.
All the actions are fully customizable in terms of distance and angle, except for the \stopac action, which does not require any additional parameters since it just stops the robot.
This package has been designed as a \acrshort{ros} service, so the communication between the visual\_semantic\_navigation package and the discrete\_move package is done synchronously and bidirectionally.
That way, the visual\_semantic\_navigation package can wait for the response of the discrete\_move package when the action has been completed before sending any new action request.
The discrete\_move server is in charge of sending the right \textit{/cmd\_vel} commands to the robot\_api package, so the robot can execute the requested action and receive the \textit{/odometry} topic information from the robot\_api package.
That way, it can calculate the movement done by the robot and stop the action when the requested action has been finished.

Embodied \acrshort{ai} navigation environments, such as Habitat~\cite{NEURIPS2021_021bbc7e}, are simulation environments where there are no movement errors in the agents.
However, the scenario in question is the real world, with real robots.
Therefore, ROS4VSN must integrate error control strategies.
To achieve this, the discrete\_move package includes two error correction strategies: one for the turn error $\epsilon_{turn}$ and one for the move straight error $\epsilon_{straight}$.

The turn error is calculated in degrees as follows,
\begin{equation}
    \label{eq:turn_error}
    \epsilon_{turn} = (\alpha_{target}- \alpha_{current}) \bmod(360)\; ,
\end{equation}
where $\alpha_{target}$ is the target orientation and $\alpha_{current}$ is the current orientation of the robot.

The move straight error is computed as:
\begin{equation}
    \label{eq:error_move}
    \epsilon_{straight} = {d - \sqrt{(x-x_{init})^2+(y-y_{init})^2}}\; ,
\end{equation}
where $d$ is the displacement distance requested by the system, $(x,y)$ encodes the current position of the robot, and $(x_{init},y_{init})$ defines the initial position of the robot.
A successful turn happens when $\epsilon_{turn}$ is less than 0.1 degrees and a successful displacement when $\epsilon_{straight}$ is less than 5 millimeters.
The ROS4VSN architecture continuously measures these errors to adapt the rotation and displacement movements of the robots, ensuring that they occur with the highest possible precision.

\paragraph*{\textbf{Acceleration and braking control}}\label{par:start-and-brake-control}

When developing a navigation system based on discrete commands, it is crucial to implement appropriate braking and acceleration mechanisms to achieve smooth and efficient robot navigation.

The package includes an implementation that combines a constant acceleration until the desired maximum speed is reached, with a deceleration phase to stop the robot.

The smoothness of the movement and the time needed to complete it depend on the percentage of the path in which the robot is accelerating and decelerating, as well as on the initial speed.
By properly adjusting these parameters, a smoother movement and a more efficient navigation time can be achieved.
Figure~\ref{fig:acceleration_stop} illustrates the total distance that the robot must travel for a \moveforward (or \movebackward) command.
This distance is divided so that the robot performs the following phases: acceleration, displacement at constant speed, and deceleration.
The instantaneous speed of the robot is controlled so that during the first phase, there is a uniformly accelerated motion, according to the following equation: $v = \sqrt{v_{init}^2 + 2 a \epsilon_{straight}}$, where $a$ is the desired acceleration, $v_{init}$ represents the initial speed of the robot, and $\epsilon_{straight}$ encodes the distance covered by the robot.
Note that $\epsilon_{straight}$ must be continuously read using the ROBOT \acrshort{api}, in order to dynamically adjust the speed.
For the deceleration stage, an equivalent negative acceleration is employed in the previous equation.
With these equations, the linear speed ($v$) that is sent to the platform can be progressively adjusted to obtain a smooth navigation.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/move_robot_acceleration}
    \caption{Acceleration and braking control scheme.}
    \label{fig:acceleration_stop}
\end{figure}

\paragraph*{\textbf{Configuration parameters}}\label{par:configuration-parameters}

The discrete move package includes the configuration file \texttt{discrete\_move.yaml}, where the different parameters used for the execution can be modified.
Table~\ref{tab:discrete_configuration} shows the configuration parameters and its default value.

\begin{table}
    \centering
    \begin{tabular}{c|c}
        \toprule
        \textit{\textbf{Parameter}}             & \textit{\textbf{Default Value}} \\
        \midrule
        Linear Velocity                         & 0.3 m/s                         \\

        Angular Velocity                        & 0.5 rad/s                       \\

        Acceleration and deceleration distances & (\moveforward distance) / 3     \\
        \bottomrule
    \end{tabular}
    \caption{Configuration parameters of discrete\_move package.}
    \label{tab:discrete_configuration}
\end{table}

\subsubsection{Visual Semantic Navigation package}\label{subsubsec:visual-semantic-navigation-package}

One of the main features of the ROS4VSN software is that it enables easy integration of different \acrshort{vsn} models independently of the robotic platform used.
To achieve this, it is essential to be agnostic with respect to the software environment needed by the particular \acrshort{vsn} model, as each model may require different dependencies.

The goal of this package is to simplify the deployment of \acrshort{vsn} models on real robots, providing an efficient software structure for the execution of these methods.
In other words, it aims to facilitate the inference tasks of discrete movement actions that these systems produce, using the state of the robotic platform.
The state is defined by the robot's position in the real world, the information provided by its sensors, and the action that was previously taken.

\acrshort{vsn} models make decisions using mainly RGB images of the environment.
However, some of them can also use additional information, such as the position and orientation of the robot, or even depth images.
Therefore, this package must be responsible for: a) capturing all the information from the sensors and processing the obtained data; b) inferring with the \acrshort{vsn} models the next navigation action; and c) communicating with the robotic platform to request the corresponding discrete motion.
This process is repeated iteratively, collecting new data, making inferences, and executing actions, until a stop condition is reached or an error occurs.

In this architecture, the package connects to the camera through the \textit{/camera} topic published by the camera\_api module, to receive the necessary RGB and depth images.
It also collects information from the robot's odometry through the \textit{/odometry} topic published by the robot\_api package.
Furthermore, it acts as a client of the discrete\_move package to send the actions determined by the \acrshort{vsn} model and receive confirmations about their execution.
This package contains two additional submodules: image preprocessing and odom processing.

\paragraph*{Image Preprocessing submodule}\label{par:image-preprocessing}

This submodule is in charge of collecting and preprocessing the images from the camera of the robot.
These images are necessary for the \acrshort{vsn} model to infer the appropriate action.
The package must be able to communicate with the camera in real time and receive its information.
This communication is done through the \textit{/camera/color} and \textit{/camera/depth} \acrshort{ros} standard topics.
Typically, depth images are taken using a time-of-flight camera.
This type of camera can lead to noise problems, including incomplete data in certain areas of the image, noise on metallic surfaces, and the impact of scene lighting on distance measurements.
To address these problems, a temporal median filter is implemented, so for a series of $N$ depth images, its noise can be reduced by discarding outliers.

\paragraph*{Odom Processing submodule}\label{par:odom-processing}

This submodule is in charge of collecting odometry information by subscribing to the \textit{/odom} topic (\textit{odometry topic}), published by the robot\_api package.
This odometry information consists of two main variables: 1) robot position, that indicates the current location of the robot with respect to its initial position; and 2) robot orientation, that defines the current direction in which the robot is oriented in relation to its initial orientation.
Some state-of-the-art \acrshort{vsn} models (\eg~\cite{ramrakhya2023}) need to input these two sources of information.

\paragraph*{\textbf{Module workflow}}\label{par:module-workflow}

The \acrshort{vsn} package uses the image submodule and the odometry submodule to capture the camera images and the robot odometry data.
This information can then be passed as input to a particular \acrshort{vsn} model.
Using the \acrshort{vsn} model integrated in the package, the next navigation action that the robot must execute is inferred.
Once the action has been determined, the package sends a message (\textit{request}) to the discrete\_move server to request the execution of the movement by the robot.
%A timer has been configured for the service to improve the user experience.
%If the client waits for 10 seconds and the service does not respond, the program stops and informs the client that the service is not working.
%This configuration allows the client to be alerted in case any problem occurs with the navigation.
After sending the request to the discrete\_move server, the package waits to receive a confirmation message.
This message indicates whether the requested action has been performed correctly or if some problem has occurred.
If for some reason an action has not been completed successfully, the server has been programmed to return $False$, which completely stops the execution of the workflow.
It is important to highlight that the workflow is repeated until either the \stopac action is inferred by the model, the time limit for the episode is reached, or the server responds with a message indicating some problem during the execution.

A configuration file is provided (\texttt{vsn.yaml}) containing default values for the parameters of the \acrshort{vsn} package.
By modifying this file, one can easily change the navigation target, the parameters associated with the median filter, or even the maximum number of steps allowed to be executed during a navigation exercise.

%\acrshort{vsn} Models modifications

\subsection{\acrshort{vsn} models}\label{subsec:vsn_models}
For this research, two state-of-the-art \acrshort{vsn} models have been selected for adaptation and integration: PIRLNav~\cite{ramrakhya2023} and VLV~\cite{chang2020}.
The first model, known as PIRLNav~\cite{ramrakhya2023}, is a \acrshort{vsn} approach that has been trained with a combination of imitation learning and a \acrshort{rl} fine-tuning.
As of today, this model reports the best results in the \acrshort{objnav}~\cite{batra2020} task in Habitat~\cite{NEURIPS2021_021bbc7e}.
The second model is the VLV approach~\cite{chang2020}, which is a \acrshort{vsn} model directly trained from YouTube videos.
The VLV model makes use of such videos to learn semantic cues for an effective navigation to semantic targets in indoor scenarios.
VLV is a modular learning solution that combines low-level and high-level navigation policies.

These models are complementary in the sense that they are based on two paradigms: a) imitation learning plus \acrshort{rl}; and b) modular learning.
This aspect allows the study, in the experimental evaluation, of which type of approach yields better results in the real world.
Note that the intention is not to retrain these models but rather to subject them to evaluation in the real world.
The aim is to analyze their generalization ability for navigation outside simulation environments.
It is precisely thanks to the ROS4VSN system that this can be done, as the technical modifications made to the models are oriented towards embedding them in a \acrshort{ros}-based system.
Next, a detailed description of the modifications and adaptations made to these models is provided so that they can be integrated into ROS4VSN and tested on real robotic platforms.

\subsubsection{VLV}

The first approach is known as Value Learning from Videos (\textsc{VLV}), developed by~\cite{chang2020}.
VLV is a modular-learning based \acrshort{vsn} model directly trained from videos of real state agencies, taken from YouTube.
In this type of video, a human records, camera in hand, the properties for sale, showcasing all the rooms they have, to generate a sort of virtual tour of the houses.
Note that the videos used do not have any type of information about the navigation actions that take place during the recording, nor in what kind of rooms or what type of objects appear.

The VLV model leverages such YouTube videos to learn semantic cues for an effective navigation to semantic targets in indoor home environments.
This way, the VLV model is trained to find in these videos a set of object categories.
Technically, the model uses pseudo action labels obtained by running an inverse model on the navigation sequences.
This inverse model is able to recognize the discrete movements that each of the transitions of the different video frames involves.
Then, the navigation policies are learned following a \acrshort{rl} approach.
VLV employs Q-learning to learn from the video sequences that have been pseudo-labeled with the actions.
The learned Q-function, and the associated value function, implicitly learn semantic cues for navigation.
In other words, the model learns what images lead to the desired category and what do not.

For the experiments presented in this work, the VLV model was embedded in the ROS4VSN architecture.
See Figure~\ref{fig:vlv_overview}.
Technically, the two navigation policies detailed in the experiments in~\cite{chang2020} that were tested in the virtual environment Habitat~\cite{NEURIPS2021_021bbc7e} were integrated.
However, the goal is now to implement them in a real robot in the real world.

This integration into the robots, using the ROS4VSN architecture, has consisted of the following steps.
First, a high-level policy that stores 12 images for each node in a topological graph (obtained by rotating 12 times by 30 degrees each) is used.
This high-level policy uses the learned value function score over these 12 images and samples the most promising direction for seeking objects of a particular object category.
VLV needs an object detector output to produce the final score for these images.
This way, the high-level policy is equipped with a mechanism to seek the object once it has been detected.
Specifically, the detector employed is the Mask R-\acrshort{cnn}~\cite{mask-rcnn}, which was embedded in the architecture as well.
To navigate, this policy uses the following discrete movements/actions: \moveforward 25 cm, \turnright, $30^\circ$ or \turnleft $30^\circ$.

These movements are compatible with the developed discrete\_move package of the ROS4VSN architecture.
Once a main direction has been chosen, the approach converts it into a short-term goal by sampling a location at an offset of 1.5 meters from the chosen node\textquotesingle s location, in the chosen view\textquotesingle s direction.
This is done using the depth-camera and a low-level navigation policy that uses occupancy maps with a fast marching planning~\cite{Sethian1996} to execute robot actions to reach the short-term goal.
These two policies have been integrated into the ROS4VSN nodes, and with them, the robot can explore the environment.

\begin{figure}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{figures/ros4vsn/vlv_diagram}
        \caption{VLV model.}
        \label{fig:vlv_overview}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{\textwidth}
        \includegraphics[width=\textwidth]{figures/ros4vsn/pirlnav_diagram}
        \caption{PIRLNav model.}
        \label{fig:pirlnav_overview}
    \end{subfigure}
    \caption{\acrshort{vsn} models integrated into the \acrshort{vsn}-ROS.}\label{fig:vsn_models_overview}
\end{figure}

\subsubsection{PIRLNav}
The second model selected for the experimental evaluation is known as \textsc{PIRLNav}~\cite{ramrakhya2023}.
As of today, this model reports the best performance in the \acrshort{objnav}~\cite{batra2020} task in Habitat~\cite{NEURIPS2021_021bbc7e}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/dataset_scene}
    \caption{One of the 3D reconstructions of the HM3D-Semantics v0.1 dataset~\cite{Ramakrishnan2021HabitatMatterport3D}, used to train PILRNav.}
    \label{fig:scene_hm3d}
\end{figure}

PILRNav is a \acrshort{vsn} approach that has been trained with a combination of imitation learning and a \acrshort{rl} fine-tuning.
The model uses \acrfull{bc} to pre-train the \acrshort{objnav} policy on a dataset of $77k$ human demonstrations, amounting more than $2370$ human annotation hours, in the \acrshort{hm3d}-Semantics v0.1 dataset~\cite{Ramakrishnan2021HabitatMatterport3D}.
This dataset provides up to 120 different 3D reconstructions of houses all around the world (see Figure~\ref{fig:scene_hm3d} for an example of one of them).
Once this \acrshort{bc} is finished, a \acrshort{rl} fine-tuning is used following the \acrshort{ddppo} approach~\cite{Wijmans2019DDPPOLN}.
The policy architecture used is a simple \acrshort{cnn} plus a \acrshort{rnn} from~\cite{yadav2022}.

In order to integrate PIRLNav into the ROS4VSN architecture, the following actions were performed.
See figure~\ref{fig:pirlnav_overview}.
The original PIRLNav needs to receive as inputs the RGB image that the agent observes, as well as the noiseless GPS and compass information offered by Habitat simulator.
GPS and compass Habitat sensor provide the agent\textquotesingle s current location and orientation information relative to the start of the episode.
In this case, because PIRLNav has to be integrated in a real robot, navigating in the real world, the model is fed with the RGB images that are acquired by the cameras in the robotic platforms.
GPS information is obtained through the odometry information provided by the robot.
For the compass, the relative orientation is recovered by analyzing all the robot's turning movements.
Note that these are not anymore noiseless sensors, as the ones used in the simulated world in which the PIRLNav model was trained.
Fortunately, no important impact on the performance of the model was observed, due to this loss of precision for these sensors.

PIRLNav is therefore integrated in the ROS4VSN architecture to control the navigation of the robot as it has been detailed.
For every captured image, as well as the GPS+compass data, the model is able to determine the next discrete movement action to be executed by the robotic platforms.
The action space used for the experiments with this model is: \moveforward 25 cm, \movebackward 25 cm, \turnright $30^\circ$, \turnleft, and \stopac.
The original PIRLNav was also trained to produce the discrete action \lookup and \lookdown, since the simulated agent could tilt its camera.
However, as it is observed in Figure~\ref{fig:robots}, in the platforms these actions are not possible.
The decision was made to replace \lookup with a \movebackward action, and \lookdown movement with the \moveforward action.
This choice is based on the reasoning that, by raising the camera, more of the scene is captured; moving the robot backward serves this purpose.
Also, since lowering the camera provides a greater level of scene detail, moving forward is considered the most appropriate choice to replace the \lookdown action.
Finally, to prevent collisions between the robot and objects in the scene, a procedure was developed that uses information from the depth image to detect obstacles at a given distance.
Note that PIRLNav does not need any low-level policy as in the VLV model.
The robot is controlled and navigates using only the set of discrete actions provided by the ROS4VSN model.


\section{Experiments}\label{sec:experiments}
This section describes the experimental evaluation designed for testing the developments in the real world.
The goal of the experimental evaluation is to answer the following question: Are the state-of-the-art \acrshort{vsn} models able to successfully operate with real robots?
The experimental setup is first detailed, where the experimental conditions and the evaluation metric are explained.
The results obtained in the real world are then analyzed.

\subsection{Experimental setup}
\label{subsec:experimental_setup}

One of the primary objectives of this chapter is to provide a comprehensive and clear protocol for the experimental evaluation of state-of-the-art \acrshort{vsn} models in real-world scenarios using real robots.
The goal is for other researchers to carry out similar experimental evaluations, using the same evaluation metrics, to facilitate comparisons of how different \acrshort{vsn} models perform in real-world navigation tasks.
For doing so, the following experimental setup is proposed.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/plano_vivienda}
    \caption{Floor plan where the experiments were performed, indicating the 15 starting positions used.}
    \label{fig:floor_plan}
\end{figure}

In a 75 m$^2$ apartment, up to 15 random starting positions were defined (see Fig.~\ref{fig:floor_plan}).
The apartment can be divided into three main areas: a bedroom, a bathroom, and a larger space that includes the kitchen, living room, and study area.
This setting contains all the object categories used to train the \acrshort{vsn} models in the experiments, such as chair, bed, plant, bathroom, monitor, table, and sofa.
It is encouraged that other researchers conduct their experiments in real-world settings that are similar in size and characteristics, allowing the robot to navigate from at least 15 different starting positions across multiple instances.

From these positions, the robot is tasked with navigating to various object categories.
Consequently, one must conduct 15 navigation experiments for each target category and measure the success of the episodes based on whether the robot reaches the designated object category in fewer than 150 discrete actions and without any collisions.
This limit of 150 steps was chosen to establish a balance between the average size of the houses typically used in Habitat~\cite{NEURIPS2021_021bbc7e} and the apartment used in the experiments.

For the evaluation metric, it is proposed to report the \acrfull{sr} of the \acrshort{vsn} models as the percentage of episodes in which navigation is deemed successful.
An experiment is considered successful if the robot halts (when the \acrshort{vsn} model samples the action \stopac) and the Euclidean distance to the target object is less than one meter.

Note that the navigation experiment mimics the evaluation performed in the \acrshort{objnav}~\cite{batra2020} task, with the same metric.
This is the standard experiment on which most \acrshort{vsn} models are compared and which currently defines the state of the art.


%Description of the robots
Two different robots were used for the experiments: a Turtlebot 2, and the LOLA2~\cite{LOLA} platform.
To do so, the ROS4VSN was embedded in both of these platforms.
This can be easily done by adapting the robot\_api module described in Section~\ref{subsubsec:robot-api2}.
Figure~\ref{fig:robots} shows the family picture of the robots invited to the experiments.
The Turtlebot 2 was mainly used for the navigation experiments in the apartment described.
The robot LOLA2 was also used in navigation experiments, but in a different location, to test the stability of the developed system and to provide a study that contains more hours of navigation, and on different platforms.
The intention in using two different platforms has been to provide evidence of the generalizability of the architecture, demonstrating that it can be tested on different robots.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth,trim=3cm 0cm 3cm 0cm, clip]{figures/ros4vsn/turtlebot_2}
        \caption{Turtlebot 2.}
        \label{fig:robot_turtlebot}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/ros4vsn/lola2}
        \caption{LOLA2 platform.}
        \label{fig:robot_lola}
    \end{subfigure}
    \caption{Pictures of the robots used in the experiments.}\label{fig:robots}
\end{figure}

When collecting information during the experiments, a procedure was developed to record relevant data for each trial.
This procedure stores the unique identifier for each episode, the sequence of actions performed, and the category of an object searched.
In addition, during the tests, qualitative information about the trajectory followed by the robot was recorded.
In particular, all the images observed by the robot during its trajectories have been saved.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/hw-sw-scheme}
    \caption{Hardware-software architecture for the development of experiments.}
    \label{fig:setup_experiment}
\end{figure}

For the experiments, the following hardware-software setup was used, as shown in Figure~\ref{fig:setup_experiment}.
The modular architecture of the developed ROS4VSN system was used to deploy it in a distributed manner.
This architecture allows separating the execution of \acrshort{ros} packages on different devices, as long as they are connected to the same network.
The robotic platforms were equipped with a laptop.
This device was used to establish the communication with the robotic platform and the camera by executing the robot\_api and camera\_api packages.
At the same time, the discrete\_move package was executed to receive the actions to be executed by the robot.
On the other hand, a workstation was used to run the \acrshort{vsn} nodes, which was equipped with an i7-1165G7 processor and an NVIDIA GeForce RTX 2060 graphics card.
Code to reproduce all the experiments is provided at \url{https://github.com/gramuah/ros4vsn}.

\subsection{\acrshort{vsn} navigation results}
\label{subsec:vsn}
The following section details the main results obtained during the navigation experiments with the robots, including both quantitative and qualitative results.

Following the experimental setup detailed in section~\ref{subsec:experimental_setup}, the following results have been obtained.
An analysis of the \acrshort{sr} for two state-of-the-art \acrshort{vsn} models, running in a Turtlebot 2 platform is provided in this study.
For every \acrshort{vsn} model, \ie VLV, and PIRLNav, their \acrshort{sr} for the different object categories they can navigate to was measured.

Table~\ref{tab:results_sota} compares the performance obtained by VLV and PIRLNav approaches when they are tested in the real world (\ie the experiments presented) and in a virtual environment (\ie the experiments reported in their respective papers).
The first observation is the difference in terms of \acrshort{sr}\@.
The \acrshort{sr} for the PIRLNav model drops from $65\%$ to $21\%$, while the VLV model loses $\sim10$ percentage points in this metric.
One of the conclusions of this study is that there is a considerable gap between the behavior of these models in the real world and in the simulation environments in which they are trained.
This indicates that further research in this direction is needed.
Interestingly, the results obtained in the real-world experiments are not consistent with the performance difference that already existed between the models in the simulation environments: VLV is the winner in the real world!
As analyzed in the discussion section below (See section~\ref{subsec:discussion}), this behavior is believed to be due to the impact of the object detector that VLV integrates, but PIRLNav does not.
While the difference between VLV and PIRLNav \acrshort{sr} in the virtual environments is of $26$ percentage points, in the real world this gap becomes of just $8$ percentage points.

\begin{table}
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Models}                       & \acrshort{sr} (Real World) & \acrshort{sr} (Virtual Environment) \\
        \midrule
        \textsc{VLV}~\cite{chang2020}         & $29.33\%$                  & $39\%$                              \\
        \textsc{PIRLNav}~\cite{ramrakhya2023} & $21.11\%$                  & $65\%$                              \\
        \bottomrule
    \end{tabular}
    \caption{Real world success rate against simulation.}
    \label{tab:results_sota}
\end{table}

The following subsections analyze in detail the results reported by each of the models.

\subsubsection{VLV results}
\label{subsubsec:vlv_results}

The VLV model analysis is presented first.
Table~\ref{tab:vlv} reports the \acrshort{sr} for every of the target categories used in the experiments.
Chairs, tables, and sofas are the categories that are easiest to navigate to.
In analyzing various trials with the robot using the VLV model with ROS4VSN, there were no successful outcomes from starting positions 10 and 12 (see Figure~\ref{fig:floor_plan}).
Additionally, only one success was observed from positions 1, 2, and 13.
Notably, the VLV implementation can reach most targets in under 60 steps.

\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textit{\textbf{Object Goal}} & \textit{\textbf{Successful episodes}} & \acrshort{sr} & \textit{\textbf{Avg. number of actions}}   \\ \midrule
        Chair                         & 6/15                                  & 40\%          & 30                                       \\
        Sofa                          & 6/15                                  & 40\%          & 65                                       \\
        Table                         & 6/15                                  & 40\%          & 42                                       \\
        Bed                           & 3/15                                  & 20\%          & 39                                       \\
        Toilet                        & 1/15                                  & 6,67\%        & 42                                       \\ \bottomrule
    \end{tabular}
    \caption{VLV \acrshort{vsn} experiment. The number of successful episodes over 15, the corresponding SR per-object goal, and the average number of actions taken to reach the target are reported.}
    \label{tab:vlv}
\end{table}

Figure~\ref{fig:vlv_qualitative} shows qualitative results for four navigation experiments with the VLV model.
Five representative images of the navigation experiments are provided.
Two successful and two unsuccessful cases are presented.
In the first experiment (first row), the robot quickly reached the Table, as the detector easily identified it in the images.
In the second experiment, the robot took a detour to the Chair because the detector failed to detect it initially.
The model predicted a point near the chair but out of view, causing the robot to move closer only after it became visible.
In the third and fourth experiments, the robot started in a challenging position in front of a refrigerator.
Due to noise in the depth image, the target point calculation was inaccurate, leading to collisions with the wall in both cases.

\begin{figure*}[t]
    \centering
    \makebox[\textwidth][c]{
        \includegraphics[width=\linewidth]{figures/ros4vsn/vlv_qualitative}}
    \caption{VLV qualitative navigation results. The first two rows show two successful cases, where the robot reached the target, while the last two rows show two situations where the navigation experiment failed.}
    \label{fig:vlv_qualitative}
\end{figure*}

\subsubsection{PIRLNav results}
\label{subsubsec:pirlnav_results}

The PIRLNav model analysis follows now in detail.
Table~\ref{tab:pirlnav} shows the \acrshort{sr} obtained for the PIRLNav model integrated into the ROS4VSN architecture.
The \acrshort{sr} reported for every object category is detailed.
The agent was able to more easily locate the most common objects in the house, such as the chair and the monitor.
The abundant and well-distributed presence of these objects facilitated the agent's work.
The model inferred the action \stopac on both objects a total of five times.
Large objects, such as the sofa and bed, showed slightly lower results.
Although easily visible from multiple locations in the dwelling, the presence of only one of these objects made it difficult for the robotic agent to spot them.
The number of times the \stopac action was sampled for these categories was substantially reduced.
With the toilet, one of the most complex challenges in navigating the robot in this home is presented.
The robot did not have full visibility of the toilet until it managed to fully enter the bathroom, having to pass through the narrow door without colliding.
Finally, for the category plant, the robot was not able to locate this category in any of the 15 attempts.
Even though the plant was visible on multiple occasions during navigation, the agent did not manage to head towards this object.
Overall, considering all the categories, the \acrshort{sr} for the PIRLNav model is of $21.11\%$.

\begin{table}[t]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textit{\textbf{Object Goal}} & \textit{\textbf{Successful episodes}} & \acrshort{sr} & \textit{\textbf{Avg. number of actions}}   \\ \midrule
        Chair                         & 5/15                                  & 33,33\%       & 49                                       \\
        Monitor                       & 5/15                                  & 33,33\%       & 91                                       \\
        Sofa                          & 5/15                                  & 33,33\%       & 70                                       \\
        Bed                           & 3/15                                  & 20,00\%       & 97                                       \\
        Toilet                        & 1/15                                  & 6,67\%        & 61                                       \\
        Plant                         & 0/15                                  & 0,00\%        & 82                                       \\ \bottomrule
    \end{tabular}
    \caption{PIRLNav \acrshort{vsn} experiment. The number of successful episodes over 15, the corresponding SR per-object goal, and the average number of actions taken to reach the target are reported.}
    \label{tab:pirlnav}
\end{table}

To conclude the analysis of the PIRLNav model, some qualitative results are provided.
Figure~\ref{fig:pirlnav_qualitative} shows the navigation trajectories for four different experiments.
Five representative images of the navigation experiments are provided.
Two successful and two unsuccessful cases are presented.

In the first experiment, the robot starts from the refrigerator and navigates through the house until it reaches the sofa.
This episode is carried out in 69 actions and ends when the model infers the action \stopac in front of the sofa.
The second experiment is also a success story, but this time the robot starts navigating from the kitchen.
The robot leaves the kitchen and navigates to the nearest chair.
This episode is performed in 36 actions and ends with the \stopac action determined by the model.
The third experiment shows a case of navigation failure, where the robot targeting the plant hits an obstacle.
In this episode, the robot navigates for 61 actions until it hits the couch.
Despite visualizing the plant from far away, when trying to approach it, the robot ends up crashing.
In the last episode, another case of navigation failure is shown while the robot was trying to make its way to the bed.
As it can be seen in this episode, the model had difficulty getting through the bathroom door without hitting itself.

\begin{figure}
    \centering
    \makebox[\textwidth][c]{
        \includegraphics[width=\linewidth]{figures/ros4vsn/pirlnav_qualitative}}
    \caption{PIRLNav qualitative navigation results. The first two rows show two successful cases, where the robot reached the target, while the last two rows show two situations where the navigation experiment failed.}
    \label{fig:pirlnav_qualitative}
\end{figure}

A video (see \url{https://youtu.be/nD0JBWNCMGg}) with more qualitative results for both of the \acrshort{vsn} models used in the experiments is provided.

\subsection{Stability analysis}
\label{subsec:marathon}

In this section, the stability of the navigation solution proposed is analyzed, showing how it can robustly navigate a considerable distance, in two different robots and over two different scenarios.
The robots successfully navigated over more than 5 kilometers in less than 38 hours in two dynamic environments.
The robots operated without direct assistance throughout the experiments, being automatically operated by the \acrshort{vsn} models integrated into the ROS4VSN software architecture.

\begin{table}
    \centering
    \begin{tabular}{l|cc}
        \toprule
        \textbf{Robot} & Time (Hours) & Distance (km) \\
        \midrule
        LOLA2          & 8            & 1.12          \\
        Turtlebot 2    & 30           & 4.10          \\\midrule
        Total          & 38           & 5.22          \\
        \bottomrule
    \end{tabular}
    \caption{Time spent and traveled distance for both robots during the experiments.}
    \label{tab:stability}
\end{table}

\subsection{Discussion}
\label{subsec:discussion}
The main question addressed with the designed experiment has been: are the state-of-the-art \acrshort{vsn} models able to successfully operate with real robots?
This implies knowing the \acrshort{sr} that these models are capable of delivering when tested in the real world and not in the virtual environments where they were trained.
Note that two \acrshort{vsn} models were selected that were originally trained with images of the real world.
The intention was to reduce as much as possible the influence of domain shift, which is known to affect artificial intelligence systems.
The study confirms that there is still room for improvement so that these models can achieve the same \acrshort{sr} in real robots.
It is expected, therefore, that the ROS4VSN library plays a fundamental role in this line of research.

Analyzing the particular behavior of the models, the following interesting discussion can be provided.
The integration of the VLV and PIRLNav models within the ROS4VSN architecture has proven to be successful.
It has resulted in a mobile agent capable of navigating in closed environments autonomously, obtaining many experiments where the robotic platforms reach the target class without complications and following logical and direct trajectories.
This navigation is comparable to that observed within simulated environments.
For the VLV model that integrates an object detector, it has been observed that this fact has a significant impact on the agent's navigation, especially when it is close to the target class.
Although the object detector does not significantly affect the general exploration, its impact becomes crucial when the robot is in the vicinity of the target.
At this crucial stage of navigation, the object detector provides a significant advantage by guiding the robot to the target more effectively.
This explains the difference of performance observed between VLV and PIRLNav in the real world.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ros4vsn/histograma_PIRLNav}
    \caption{Histogram of navigation actions sampled by PIRLNav model.}
    \label{fig:histrogram_pirlnav}
\end{figure}

In terms of qualitative aspects of navigation, it is believed that the PIRLNav model is better than the VLV model.
Note that VLV, every time the high-level policy has to make a decision, needs the robot to turn completely on itself, taking 12 captures on which it will decide which direction to move forward.
This can be observed in the provided video.
This feature slows down navigation, although it could be solved with some specific hardware.
In contrast, PIRLNav offers a more direct navigation experience.
It is interesting to observe the type of action sampling that PIRLNav performs while being executed on the robots.
Figure~\ref{fig:histrogram_pirlnav} shows a histogram corresponding to the distribution of navigation actions performed by PIRLNav.
First, one can observe that the \lookup and \lookdown actions have hardly been selected.
This allows affirming that the impact of the adaptations made to replace these actions by backward and forward movements, respectively, could hardly have had a considerable impact on the final results.
Second, the most popular actions, as they are the ones that motivate the exploration of the environment, are those of advances and turns.
The stop action was sampled 31 times.
This is a $0.2\%$ as it is reflected in the histogram provided.
It is believed that work should be done on solutions to increase the number of times the stop action is selected, but to do so reliably.

Finally, the study confirms some of the conclusions reported in recent works, \eg~\cite{gervet2022}.
Modular-learning models, such as VLV, perform better than end-to-end learning approaches, \eg~PIRLNav, when tested in the real world.


\section{Conclusions}\label{sec:conclusions_ros4vsn}
To conclude, a \acrshort{ros}-based framework for visual semantic navigation named ROS4VSN has been presented.
This framework allows easily testing and comparing different \acrshort{vsn} models in real robots.
Using ROS4VSN, it has been possible to embed two cutting-edge \acrshort{vsn} models into two distinct real robotic platforms.
The chosen models are PIRLNav~\cite{ramrakhya2023} and VLV~\cite{chang2020}.
To seamlessly integrate these models, technical modifications have been needed.
These adaptations ensure a smooth transition for the models, enabling them to shift from interacting with observations generated in simulation environments to those obtained from the real world.
A thorough experimental evaluation has been offered to showcase how these \acrshort{vsn} approaches behave when navigating in the real world.
The novel framework shows robust stability, being able to run for a considerable distance, in two different robots, without any human intervention.
The performance of state-of-the-art \acrshort{vsn} models is significantly lower in the real world than in the virtual environments where they were trained is shown by the results.
The next chapter will explore how to improve the performance of these models in the real world by alternative methods of training them.
