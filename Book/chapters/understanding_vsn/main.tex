\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{svg}
\usepackage{hyperref}

\DeclareMathOperator*{\argmax}{arg\,max}
\def\eg{\emph{e.g. }}
\def\Eg{\emph{E.g.}}
\def\etal{\emph{et al. }}
\def\ie{\emph{i.e. }}
\def\Ie{\emph{I.e. }}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Towards Clear Evaluation of Robotic Visual Semantic Navigation \\
\thanks{This research was funded by projects: AIRPLANE, with reference PID2019-104323RB-C31; EYEOT, with reference PID2021-128362OB-100; and POLLUTWIN, with reference TED2021-129162B-C22 from the Ministry of Science and Innovation of Spain.}
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address}
% }

\author{
\IEEEauthorblockN{
Carlos Guti\' errez-\'Alvarez\IEEEauthorrefmark{1}, Sergio Hern\'andez-Garc\'ia\IEEEauthorrefmark{2}, Nadia Nasri\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}, \\ Alfredo Cuesta-Infante\IEEEauthorrefmark{2} and Roberto J. L\'opez-Sastre\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}University of Alcal\'a, Department of Signal Theory and Communications, Alcal\'a de Henares, Spain\\
Email: \{carlos.gutierrezalva, nadia.nasri, robertoj.lopez\}@uah.es}
 \IEEEauthorblockA{\IEEEauthorrefmark{2}Rey Juan Carlos University, Superior Polytechnic School of Computer Science, M\'ostoles, Spain\\
Email: \{sergio.hernandez, alfredo.cuesta\}@urjc.es}
\IEEEauthorblockA{\IEEEauthorrefmark{3}University of Alicante, Institute for Computer Research, Alicante, Spain}
}


\maketitle

\begin{abstract}
In this paper we address the problem of visual semantic navigation (VSN), in which a robot needs to navigate through an environment to reach an object having only access to egocentric RGB perception sensors.
This is a recently explored problem, where most of the approaches leverage last advances in deep learning models for visual perception, combined with reinforcement learning (RL) strategies.
Nonetheless, after a review of the literature, it is complicated to perform direct comparisons between the different solutions.
The main difficulties lie in the fact that the navigation environments in which the experimental metrics are reported are not accessible, and each approach uses different RL libraries.
In this paper, we release a publicly available experimental setup for the VSN problem, with the aim of providing a clear benchmark.
It has been constructed using pyRIL, an open source python library for RL, and two navigation environments: Miniwolrd-Maze from gym-miniworld, and one 3D scene from HM3D dataset using AI Habitat simulator.
We finally propose a state-of-the-art VSN model, consisting in a Contrastive Language Image Pretraining (CLIP) visual encoder plus a set of two recurrent neural networks for producing the discrete navigation actions.
This model is evaluated in the proposed experimental setup, with a careful analysis of the main VSN challenges, namely: the sparse rewards problem; and the exploitation-exploration trade-off.
Code is available at: \url{https://github.com/gramuah/vsn}.
\end{abstract}

\begin{IEEEkeywords}
navigation, reinforcement learning, robot, deep learning
\end{IEEEkeywords}

\input{introduction.tex}

\input{related_work.tex}

\input{rl_navigation.tex}

\input{experiments.tex}

\input{conclusions.tex}

% \section*{Acknowledgment}
% This research was funded by project AIRPLANE, with reference PID2019-104323RB-C31, from the Ministry of Science and Innovation of Spain.

\bibliographystyle{IEEEtran}
\bibliography{library}

\end{document}
